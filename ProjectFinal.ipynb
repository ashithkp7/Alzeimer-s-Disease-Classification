{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuWvuA+S/SqAC/MgCuOc3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashithkp7/Alzeimer-s-Disease-Classification/blob/main/ProjectFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1xMZMpUgJS",
        "outputId": "25396fd7-a8d9-4c91-944e-4a4a57a20f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Xg8Nfnd9yE",
        "outputId": "9c9485aa-7409-41bc-b766-84977a3d25f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels: {'AD': 0, 'CN': 1, 'MCI': 2} Total groups: 802\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 125MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4 train_loss=0.6199 val_acc=0.7273\n",
            "Epoch 2/4 train_loss=0.3143 val_acc=0.7603\n",
            "Epoch 3/4 train_loss=0.2242 val_acc=0.8430\n",
            "Epoch 4/4 train_loss=0.1515 val_acc=0.9504\n",
            "Test acc: 0.9629629629629629\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9310    0.9643    0.9474        28\n",
            "           1     0.9750    0.9512    0.9630        41\n",
            "           2     1.0000    1.0000    1.0000        12\n",
            "\n",
            "    accuracy                         0.9630        81\n",
            "   macro avg     0.9687    0.9718    0.9701        81\n",
            "weighted avg     0.9635    0.9630    0.9631        81\n",
            "\n",
            "Done. Outputs in: ./slice_attention_output\n"
          ]
        }
      ],
      "source": [
        "# train_slice_attention.py\n",
        "import os, random, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Project_A\"\n",
        "OUT_DIR = \"./slice_attention_output\"\n",
        "MODE = \"images\"\n",
        "NUM_SLICES = 8\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 4\n",
        "LR = 1e-4\n",
        "RANDOM_SEED = 42\n",
        "NUM_WORKERS = 2\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_PRETRAINED = True\n",
        "SAVE_BEST = True\n",
        "\n",
        "SAVE_ATTENTION_PRINTS = False\n",
        "VOLUMES_ROOT = os.path.join(DATA_ROOT, \"Volumes\")\n",
        "IMAGES_ROOT = os.path.join(DATA_ROOT, \"OriginalDataset\")\n",
        "FOLDER_TO_LABEL = {\n",
        "    \"NonDemented\": \"CN\",\n",
        "    \"MildDemented\": \"MCI\",\n",
        "    \"ModerateDemented\": \"MCI\",\n",
        "    \"VeryMildDemented\": \"AD\"\n",
        "}\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
        "\n",
        "def find_volume_file(subject_folder):\n",
        "    p = Path(subject_folder)\n",
        "    if not p.exists(): return None\n",
        "    patterns = [\"**/*.nii\", \"**/*.nii.gz\", \"**/*.img\", \"**/*.hdr\", \"*.nii\", \"*.nii.gz\"]\n",
        "    for pat in patterns:\n",
        "        found = list(p.glob(pat))\n",
        "        if found:\n",
        "            found_sorted = sorted(found, key=lambda x: x.stat().st_size, reverse=True)\n",
        "            return str(found_sorted[0].resolve())\n",
        "    return None\n",
        "\n",
        "\n",
        "def load_volume_as_numpy(path):\n",
        "    img = nib.load(path)\n",
        "    data = img.get_fdata()\n",
        "    data = np.nan_to_num(data.astype(np.float32))\n",
        "    m = np.mean(data); s = np.std(data) + 1e-8\n",
        "    data = (data - m) / s\n",
        "    return data\n",
        "\n",
        "\n",
        "def select_slices_from_volume(vol, num_slices=8, axis=2, mode='uniform'):\n",
        "    vol = np.moveaxis(vol, axis, -1)\n",
        "    n = vol.shape[-1]\n",
        "    if n == 0: return []\n",
        "    if n <= num_slices:\n",
        "        idxs = list(range(n))\n",
        "    else:\n",
        "        if mode=='uniform':\n",
        "            step = n / num_slices\n",
        "            idxs = [int(i*step) for i in range(num_slices)]\n",
        "        elif mode=='center':\n",
        "            c = n//2; half = num_slices//2; start = max(0, c-half); idxs = list(range(start, start+num_slices))\n",
        "        else:\n",
        "            idxs = sorted(random.sample(range(n), num_slices))\n",
        "    slices = [np.atleast_2d(np.squeeze(vol[..., i])) for i in idxs]\n",
        "    return slices\n",
        "\n",
        "\n",
        "def build_groups_from_images(images_root, num_slices=8):\n",
        "    class2files = {}\n",
        "    root = Path(images_root)\n",
        "    for cls_dir in sorted(root.iterdir()):\n",
        "        if not cls_dir.is_dir(): continue\n",
        "        if cls_dir.name not in FOLDER_TO_LABEL: continue\n",
        "        label = FOLDER_TO_LABEL[cls_dir.name]\n",
        "        files = []\n",
        "        for ext in IMAGE_EXTS:\n",
        "            files += sorted([str(p) for p in cls_dir.rglob(f\"*{ext}\")])\n",
        "        if len(files)==0: continue\n",
        "        class2files.setdefault(label, []).extend(files)\n",
        "    groups=[]\n",
        "    label2idx = {c:i for i,c in enumerate(sorted(class2files.keys()))}\n",
        "    for cls, fls in class2files.items():\n",
        "        chunks = [fls[i:i+num_slices] for i in range(0, len(fls), num_slices)]\n",
        "        for i,chunk in enumerate(chunks):\n",
        "            groups.append({\"group_id\": f\"{cls}_g{i:04d}\", \"paths\": chunk, \"label_name\": cls, \"label_idx\": label2idx[cls]})\n",
        "    return groups, label2idx\n",
        "\n",
        "\n",
        "def build_groups_from_volumes(volumes_root, num_slices=8):\n",
        "    groups=[]\n",
        "    for subj in sorted(Path(volumes_root).iterdir()):\n",
        "        if not subj.is_dir(): continue\n",
        "        volfile = find_volume_file(subj)\n",
        "        if volfile is None: continue\n",
        "        try:\n",
        "            vol = load_volume_as_numpy(volfile)\n",
        "        except Exception as e:\n",
        "            print(\"fail load\", volfile, e); continue\n",
        "        slices = select_slices_from_volume(vol, num_slices=num_slices, axis=2, mode='uniform')\n",
        "        if len(slices)==0: continue\n",
        "        name = subj.name.upper()\n",
        "        label_guess = \"unknown\"\n",
        "        if \"AD\" in name or \"ALZ\" in name: label_guess = \"AD\"\n",
        "        elif \"MCI\" in name: label_guess = \"MCI\"\n",
        "        elif \"CN\" in name or \"NORMAL\" in name or \"NON\" in name: label_guess = \"CN\"\n",
        "        groups.append({\"group_id\": subj.name, \"slices\": slices, \"label_name\": label_guess})\n",
        "    label_names = sorted(list({g['label_name'] for g in groups}))\n",
        "    label2idx = {n:i for i,n in enumerate(label_names)}\n",
        "    for g in groups: g['label_idx'] = label2idx[g['label_name']]\n",
        "    return groups, label2idx\n",
        "\n",
        "class ImageGroupDataset(Dataset):\n",
        "    def __init__(self, groups, num_slices=8, transform=None, image_size=224):\n",
        "        self.groups = groups; self.num_slices=num_slices; self.transform=transform; self.image_size=image_size\n",
        "    def __len__(self): return len(self.groups)\n",
        "    def sample_slices(self, paths):\n",
        "        n=len(paths)\n",
        "        if n >= self.num_slices:\n",
        "            step = n / self.num_slices\n",
        "            idxs = [int(i*step) for i in range(self.num_slices)]\n",
        "        else:\n",
        "            idxs = [random.randrange(n) for _ in range(self.num_slices)]\n",
        "        return [paths[i] for i in idxs]\n",
        "    def __getitem__(self, idx):\n",
        "        g=self.groups[idx]; paths=g['paths']; sel=self.sample_slices(paths)\n",
        "        imgs=[]\n",
        "        for p in sel:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            if self.transform: img = self.transform(img)\n",
        "            imgs.append(img)\n",
        "        x = torch.stack(imgs, dim=0)\n",
        "        return x, int(g['label_idx']), g['group_id']\n",
        "\n",
        "class VolumeGroupDataset(Dataset):\n",
        "    def __init__(self, groups, num_slices=8, transform=None, image_size=224):\n",
        "        self.groups=groups; self.num_slices=num_slices; self.transform=transform; self.image_size=image_size\n",
        "    def __len__(self): return len(self.groups)\n",
        "    def sample_slices(self, slices_list):\n",
        "        n=len(slices_list)\n",
        "        if n >= self.num_slices:\n",
        "            step = n/self.num_slices\n",
        "            idxs=[int(i*step) for i in range(self.num_slices)]\n",
        "        else:\n",
        "            idxs=[random.randrange(n) for _ in range(self.num_slices)]\n",
        "        return [slices_list[i] for i in idxs]\n",
        "    def __getitem__(self, idx):\n",
        "        g=self.groups[idx]; slices_list=g['slices']; sel=self.sample_slices(slices_list)\n",
        "        imgs=[]\n",
        "        for sl in sel:\n",
        "            mn, mx = float(np.min(sl)), float(np.max(sl))\n",
        "            if mx - mn < 1e-6:\n",
        "                arr = np.zeros_like(sl, dtype=np.uint8)\n",
        "            else:\n",
        "                sln = (sl - mn) / (mx-mn)\n",
        "                arr = (sln*255.0).astype(np.uint8)\n",
        "            pil = Image.fromarray(arr).convert(\"RGB\")\n",
        "            if self.transform: pil = self.transform(pil)\n",
        "            imgs.append(pil)\n",
        "        x = torch.stack(imgs, dim=0)\n",
        "        return x, int(g['label_idx']), g['group_id']\n",
        "\n",
        "class SliceAttentionModel(nn.Module):\n",
        "    def __init__(self, pretrained=True, embedding_dim=512, num_classes=3):\n",
        "        super().__init__()\n",
        "        if hasattr(models, \"ResNet18_Weights\") and pretrained:\n",
        "            res = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            res = models.resnet18(pretrained=pretrained)\n",
        "        features = list(res.children())[:-1]\n",
        "        self.backbone = nn.Sequential(*features)\n",
        "        self.embedding_dim = res.fc.in_features\n",
        "        self.att_mlp = nn.Sequential(nn.Linear(self.embedding_dim,128), nn.ReLU(), nn.Linear(128,1))\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.embedding_dim,256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,num_classes))\n",
        "    def forward(self,x):\n",
        "        B,S,C,H,W = x.shape\n",
        "        x = x.view(B*S, C, H, W)\n",
        "        feats = self.backbone(x)\n",
        "        feats = feats.view(B, S, self.embedding_dim)\n",
        "        scores = self.att_mlp(feats).squeeze(-1)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        fused = (feats * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out, weights\n",
        "\n",
        "def visualize_attention_stack(x_tensor, weights, out_path):\n",
        "    inv_norm = T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "                           std=[1/0.229,1/0.224,1/0.225])\n",
        "    x_vis = inv_norm(x_tensor).clamp(0,1)\n",
        "    to_pil = T.ToPILImage()\n",
        "    S = x_vis.shape[0]\n",
        "    fig, axes = plt.subplots(1, S+1, figsize=(3*(S+1),3))\n",
        "    axes[0].bar(range(S), weights); axes[0].set_title(\"weights\")\n",
        "    for i in range(S):\n",
        "        axes[i+1].imshow(to_pil(x_vis[i].cpu()).convert('L')); axes[i+1].axis('off'); axes[i+1].set_title(f\"{weights[i]:.2f}\")\n",
        "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
        "\n",
        "def main():\n",
        "    if MODE == \"nifti\":\n",
        "        groups, label2idx = build_groups_from_volumes(VOLUMES_ROOT, num_slices=NUM_SLICES)\n",
        "    else:\n",
        "        groups, label2idx = build_groups_from_images(IMAGES_ROOT, num_slices=NUM_SLICES)\n",
        "    print(\"Labels:\", label2idx, \"Total groups:\", len(groups))\n",
        "    if len(groups) < 4:\n",
        "        print(\"Warning: not enough groups.\")\n",
        "\n",
        "    df = pd.DataFrame(groups)\n",
        "    trainval, test = train_test_split(df, test_size=0.10, stratify=df['label_idx'], random_state=RANDOM_SEED)\n",
        "    rel_val = 0.15 / (1 - 0.10)\n",
        "    train, val = train_test_split(trainval, test_size=rel_val, stratify=trainval['label_idx'], random_state=RANDOM_SEED)\n",
        "    train_g = train.to_dict(orient='records'); val_g = val.to_dict(orient='records'); test_g = test.to_dict(orient='records')\n",
        "    pd.DataFrame(train_g).to_csv(os.path.join(OUT_DIR,\"train_groups.csv\"), index=False)\n",
        "    pd.DataFrame(val_g).to_csv(os.path.join(OUT_DIR,\"val_groups.csv\"), index=False)\n",
        "    pd.DataFrame(test_g).to_csv(os.path.join(OUT_DIR,\"test_groups.csv\"), index=False)\n",
        "\n",
        "    train_transform = T.Compose([T.Resize((IMAGE_SIZE,IMAGE_SIZE)), T.RandomHorizontalFlip(), T.RandomRotation(8),\n",
        "                                 T.ColorJitter(0.08,0.08), T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "    val_transform = T.Compose([T.Resize((IMAGE_SIZE,IMAGE_SIZE)), T.ToTensor(), T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "\n",
        "    if MODE==\"nifti\":\n",
        "        train_ds = VolumeGroupDataset(train_g, num_slices=NUM_SLICES, transform=train_transform, image_size=IMAGE_SIZE)\n",
        "        val_ds = VolumeGroupDataset(val_g, num_slices=NUM_SLICES, transform=val_transform, image_size=IMAGE_SIZE)\n",
        "        test_ds = VolumeGroupDataset(test_g, num_slices=NUM_SLICES, transform=val_transform, image_size=IMAGE_SIZE)\n",
        "    else:\n",
        "        train_ds = ImageGroupDataset(train_g, num_slices=NUM_SLICES, transform=train_transform, image_size=IMAGE_SIZE)\n",
        "        val_ds = ImageGroupDataset(val_g, num_slices=NUM_SLICES, transform=val_transform, image_size=IMAGE_SIZE)\n",
        "        test_ds = ImageGroupDataset(test_g, num_slices=NUM_SLICES, transform=val_transform, image_size=IMAGE_SIZE)\n",
        "\n",
        "    train_labels = np.array([g['label_idx'] for g in train_g])\n",
        "    class_counts = np.bincount(train_labels)\n",
        "    class_counts = np.where(class_counts==0, 1, class_counts)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    samples_weight = class_weights[train_labels]\n",
        "    sampler = WeightedRandomSampler(torch.from_numpy(samples_weight).float(), num_samples=len(samples_weight), replacement=True)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    model = SliceAttentionModel(pretrained=USE_PRETRAINED, embedding_dim=512, num_classes=len(label2idx)).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(DEVICE))\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_path = os.path.join(OUT_DIR, \"best_slice_attn.pt\")\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X, y, gids in train_loader:\n",
        "            X = X.to(DEVICE).float(); y = y.to(DEVICE).long()\n",
        "            opt.zero_grad()\n",
        "            out, _ = model(X)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        preds, trues = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, gids in val_loader:\n",
        "                X = X.to(DEVICE).float(); y = y.to(DEVICE).long()\n",
        "                out, _ = model(X)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy().tolist())\n",
        "                trues.extend(y.cpu().numpy().tolist())\n",
        "        val_acc = accuracy_score(trues, preds) if len(trues) else 0.0\n",
        "        print(f\"Epoch {epoch}/{NUM_EPOCHS} train_loss={train_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            if SAVE_BEST:\n",
        "                torch.save(model.state_dict(), best_path)\n",
        "\n",
        "    if SAVE_BEST and os.path.exists(best_path):\n",
        "        model.load_state_dict(torch.load(best_path))\n",
        "    model.eval()\n",
        "    preds, trues, gids = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y, group_ids in test_loader:\n",
        "            X = X.to(DEVICE).float()\n",
        "            out, w = model(X)\n",
        "            preds.extend(out.argmax(dim=1).cpu().tolist())\n",
        "            trues.extend(y.cpu().tolist())\n",
        "            gids.extend(group_ids)\n",
        "    print(\"Test acc:\", accuracy_score(trues, preds))\n",
        "    print(classification_report(trues, preds, digits=4))\n",
        "    cm = confusion_matrix(trues, preds)\n",
        "    np.savetxt(os.path.join(OUT_DIR, \"confusion_matrix.csv\"), cm, delimiter=\",\")\n",
        "\n",
        "    vis_dir = os.path.join(OUT_DIR, \"attention_viz\"); os.makedirs(vis_dir, exist_ok=True)\n",
        "    nvis = min(12, len(test_g))\n",
        "    for i in range(nvis):\n",
        "        X, y, gid = test_ds[i]\n",
        "        xb = X.unsqueeze(0).to(DEVICE).float()\n",
        "        with torch.no_grad():\n",
        "            out, weights = model(xb)\n",
        "        weights_np = weights.cpu().numpy().squeeze()\n",
        "        inv_norm = T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229,1/0.224,1/0.225])\n",
        "        X_vis = inv_norm(X)\n",
        "\n",
        "        visualize_attention_stack(X_vis, weights_np, os.path.join(vis_dir, f\"{gid}_attn.png\"))\n",
        "\n",
        "        used_paths = None\n",
        "        try:\n",
        "            used_paths = test_g[i].get('paths', None)\n",
        "        except Exception:\n",
        "            used_paths = None\n",
        "\n",
        "        to_pil = T.ToPILImage()\n",
        "\n",
        "        if used_paths and SAVE_ATTENTION_PRINTS:\n",
        "            paths_all = used_paths\n",
        "            n_all = len(paths_all)\n",
        "            if n_all >= NUM_SLICES:\n",
        "                step = n_all / NUM_SLICES\n",
        "                idxs = [int(j*step) for j in range(NUM_SLICES)]\n",
        "            else:\n",
        "                idxs = list(range(n_all))\n",
        "            selected_paths = [paths_all[k] for k in idxs]\n",
        "            for idx, (p, w) in enumerate(zip(selected_paths, weights_np)):\n",
        "                print(f\"Slice {idx+1}: {p}  --> Attention Weight = {w:.4f}\")\n",
        "            order = np.argsort(weights_np)[::-1]\n",
        "            print(\"\\n=== Ranking (most -> least) ===\")\n",
        "            for rank, idx in enumerate(order, 1):\n",
        "                print(f\"Rank {rank}: {selected_paths[idx]}  (Weight = {weights_np[idx]:.4f})\")\n",
        "            best_idx = order[0]\n",
        "            best_path = selected_paths[best_idx]\n",
        "            print(\"\\n*** MOST INFORMATIVE SLICE ***\")\n",
        "            print(f\"Image: {best_path}\")\n",
        "            print(f\"Attention Weight: {weights_np[best_idx]:.4f}\\n\")\n",
        "            try:\n",
        "                best_img = to_pil(X_vis[best_idx].cpu())\n",
        "                best_img.save(os.path.join(vis_dir, f\"{gid}_best_slice.png\"))\n",
        "            except Exception as e:\n",
        "                print(\"Could not save best slice image from X_vis:\", e)\n",
        "        else:\n",
        "            order = np.argsort(weights_np)[::-1]\n",
        "            best_idx = order[0]\n",
        "            try:\n",
        "                best_img = to_pil(X_vis[best_idx].cpu())\n",
        "                best_img.save(os.path.join(vis_dir, f\"{gid}_best_slice.png\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    print(\"Done. Outputs in:\", OUT_DIR)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNk9NnJo3-Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inference_slice_attention.py\n",
        "\n",
        "import os, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TEST_FOLDER = \"/content/drive/MyDrive/Project_A/Test_images/\"\n",
        "NIFTI_FILE = \"\"\n",
        "MODEL_PATH = \"./slice_attention_output/best_slice_attn.pt\"\n",
        "OUT_DIR = \"./inference_outputs\"\n",
        "NUM_SLICES = 8\n",
        "IMAGE_SIZE = 224\n",
        "MEAN = [0.485,0.456,0.406]; STD = [0.229,0.224,0.225]\n",
        "\n",
        "idx2label = {0:\"AD\", 1:\"CN\", 2:\"MCI\"}\n",
        "IMAGE_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "transform = T.Compose([T.Resize((IMAGE_SIZE,IMAGE_SIZE)), T.ToTensor(), T.Normalize(MEAN,STD)])\n",
        "\n",
        "class SliceAttentionModel(nn.Module):\n",
        "    def __init__(self, pretrained=False, embedding_dim=512, num_classes=3):\n",
        "        super().__init__()\n",
        "        if hasattr(models, \"ResNet18_Weights\") and pretrained:\n",
        "            res = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            res = models.resnet18(pretrained=pretrained)\n",
        "        layers = list(res.children())[:-1]\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "        self.embedding_dim = res.fc.in_features\n",
        "        self.att_mlp = nn.Sequential(nn.Linear(self.embedding_dim,128), nn.ReLU(), nn.Linear(128,1))\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.embedding_dim,256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256,num_classes))\n",
        "    def forward(self, x):\n",
        "        B,S,C,H,W = x.shape\n",
        "        x = x.view(B*S, C, H, W)\n",
        "        feats = self.backbone(x)\n",
        "        feats = feats.view(B, S, self.embedding_dim)\n",
        "        scores = self.att_mlp(feats).squeeze(-1)\n",
        "        weights = torch.softmax(scores, dim=1)\n",
        "        fused = (feats * weights.unsqueeze(-1)).sum(dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out, weights\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def build_stack_from_folder(folder, num_slices=NUM_SLICES):\n",
        "    files = sorted([str(p) for p in Path(folder).glob(\"*\") if p.suffix.lower() in IMAGE_EXTS])\n",
        "    if len(files) == 0:\n",
        "        raise ValueError(\"No images found in folder: \" + str(folder))\n",
        "    n = len(files)\n",
        "\n",
        "    if n < num_slices:\n",
        "        selected = files[:]\n",
        "    else:\n",
        "        step = n / num_slices\n",
        "        idxs = [int(i*step) for i in range(num_slices)]\n",
        "        selected = [files[i] for i in idxs]\n",
        "\n",
        "    tensors = []\n",
        "    for p in selected:\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        tensors.append(transform(img))\n",
        "    stack = torch.stack(tensors, dim=0)\n",
        "    return stack, selected\n",
        "\n",
        "\n",
        "def extract_slices_from_nifti(nifti_path, num_slices=NUM_SLICES, axis=2):\n",
        "    img = nib.load(nifti_path)\n",
        "    vol = img.get_fdata().astype(float)\n",
        "    vol = np.nan_to_num(vol)\n",
        "    vol = (vol - vol.mean()) / (vol.std() + 1e-8)\n",
        "    vol = np.moveaxis(vol, axis, -1)\n",
        "    n = vol.shape[-1]\n",
        "    if n == 0: raise ValueError(\"No slices in NIfTI\")\n",
        "    if n >= num_slices:\n",
        "        step = n / num_slices\n",
        "        idxs = [int(i*step) for i in range(num_slices)]\n",
        "    else:\n",
        "        idxs = [min(n-1, int(i)) for i in range(num_slices)]\n",
        "    slices = []\n",
        "    for i in idxs:\n",
        "        sl = vol[..., i]\n",
        "        mn, mx = float(sl.min()), float(sl.max())\n",
        "        if mx-mn < 1e-6:\n",
        "            arr = np.zeros((sl.shape[0], sl.shape[1]), dtype=np.uint8)\n",
        "        else:\n",
        "            sln = (sl - mn) / (mx - mn)\n",
        "            arr = (sln * 255.0).astype(np.uint8)\n",
        "        pil = Image.fromarray(arr).convert(\"RGB\")\n",
        "        slices.append(transform(pil))\n",
        "    stack = torch.stack(slices, dim=0)\n",
        "    return stack, idxs\n",
        "\n",
        "\n",
        "def save_topk(stack, weights, prefix, paths=None, topk=6):\n",
        "    \"\"\"\n",
        "    stack: tensor (S,C,H,W) before batch dim and before normalization inversion\n",
        "    weights: 1D numpy array of length S\n",
        "    paths: optional list of original filenames corresponding to stack order\n",
        "    \"\"\"\n",
        "    inv = T.Normalize(mean=[-m/s for m,s in zip(MEAN,STD)], std=[1/s for s in STD])\n",
        "    x_vis = inv(stack).clamp(0,1)\n",
        "    to_pil = T.ToPILImage()\n",
        "    order = np.argsort(weights)[::-1]\n",
        "    S = len(weights)\n",
        "    topk = min(topk, S)\n",
        "    for r, idx in enumerate(order[:topk], start=1):\n",
        "        pil = to_pil(x_vis[idx].cpu())\n",
        "        if paths:\n",
        "            base = os.path.basename(paths[idx])\n",
        "            name = f\"{os.path.splitext(base)[0]}\"\n",
        "        else:\n",
        "            name = f\"slice{idx}\"\n",
        "        pil.save(os.path.join(OUT_DIR, f\"{prefix}_rank{r}_{name}_w{weights[idx]:.4f}.png\"))\n",
        "    plt.figure(figsize=(6,2)); plt.bar(range(S), weights); plt.title(\"attention weights\")\n",
        "    plt.savefig(os.path.join(OUT_DIR, f\"{prefix}_weights.png\")); plt.close()\n",
        "\n",
        "def run_inference_from_folder(folder):\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}. Train and save the model first or set MODEL_PATH correctly.\")\n",
        "    stack, used_paths = build_stack_from_folder(folder)\n",
        "    xb = stack.unsqueeze(0).to(DEVICE)\n",
        "    model = SliceAttentionModel(pretrained=False, embedding_dim=512, num_classes=len(idx2label)).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out, weights = model(xb)\n",
        "        probs = torch.softmax(out, dim=1).cpu().numpy().squeeze()\n",
        "        weights_np = weights.cpu().numpy().squeeze()\n",
        "    pred_idx = int(np.argmax(probs)); pred_label = idx2label[pred_idx]\n",
        "    save_topk(stack, weights_np, \"subject_folder\", paths=used_paths, topk=min(6, len(weights_np)))\n",
        "\n",
        "    print(\"\\n=== Slice Attention Ranking ===\")\n",
        "    for i, (p,w) in enumerate(zip(used_paths, weights_np), start=1):\n",
        "        print(f\"Slice {i}: {p}  --> Attention Weight = {w:.4f}\")\n",
        "    order = np.argsort(weights_np)[::-1]\n",
        "    print(\"\\n=== Ranking (most -> least) ===\")\n",
        "    for rank, idx in enumerate(order, start=1):\n",
        "        print(f\"Rank {rank}: {used_paths[idx]}  (Weight = {weights_np[idx]:.4f})\")\n",
        "    best_idx = order[0]\n",
        "    print(\"\\n*** MOST INFORMATIVE SLICE ***\")\n",
        "    print(f\"Image: {used_paths[best_idx]}\")\n",
        "    print(f\"Attention Weight: {weights_np[best_idx]:.4f}\\n\")\n",
        "    return {\"pred_label\": pred_label, \"probs\": probs, \"weights\": weights_np, \"used_paths\": used_paths}\n",
        "\n",
        "\n",
        "def run_inference_from_nifti(nifti_path):\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(f\"Model not found at {MODEL_PATH}. Train and save the model first or set MODEL_PATH correctly.\")\n",
        "    stack, slice_idxs = extract_slices_from_nifti(nifti_path)\n",
        "    xb = stack.unsqueeze(0).to(DEVICE)\n",
        "    model = SliceAttentionModel(pretrained=False, embedding_dim=512, num_classes=len(idx2label)).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out, weights = model(xb)\n",
        "        probs = torch.softmax(out, dim=1).cpu().numpy().squeeze()\n",
        "        weights_np = weights.cpu().numpy().squeeze()\n",
        "    pred_idx = int(np.argmax(probs)); pred_label = idx2label[pred_idx]\n",
        "    save_topk(stack, weights_np, \"subject_nifti\", paths=None, topk=min(6, len(weights_np)))\n",
        "    print(\"\\n=== Slice Attention Ranking (by index) ===\")\n",
        "    for i, w in enumerate(weights_np):\n",
        "        print(f\"Slice index {slice_idxs[i]}  --> Attention Weight = {w:.4f}\")\n",
        "    order = np.argsort(weights_np)[::-1]\n",
        "    print(\"\\n=== Ranking (most -> least) ===\")\n",
        "    for rank, idx in enumerate(order, start=1):\n",
        "        print(f\"Rank {rank}: slice index {slice_idxs[idx]}  (Weight = {weights_np[idx]:.4f})\")\n",
        "    best_idx = order[0]\n",
        "    print(\"\\n*** MOST INFORMATIVE SLICE (index) ***\")\n",
        "    print(f\"Slice index: {slice_idxs[best_idx]}, Weight: {weights_np[best_idx]:.4f}\\n\")\n",
        "    return {\"pred_label\": pred_label, \"probs\": probs, \"weights\": weights_np, \"slice_indices\": slice_idxs}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if TEST_FOLDER and Path(TEST_FOLDER).exists():\n",
        "        res = run_inference_from_folder(TEST_FOLDER)\n",
        "        print(\"Folder prediction:\", res[\"pred_label\"])\n",
        "        print(\"Probabilities:\", res[\"probs\"])\n",
        "        print(\"Saved attention images to:\", OUT_DIR)\n",
        "    elif NIFTI_FILE and Path(NIFTI_FILE).exists():\n",
        "        res = run_inference_from_nifti(NIFTI_FILE)\n",
        "        print(\"NIfTI prediction:\", res[\"pred_label\"])\n",
        "        print(\"Probabilities:\", res[\"probs\"])\n",
        "        print(\"Saved attention images to:\", OUT_DIR)\n",
        "    else:\n",
        "        print(\"No valid TEST_FOLDER or NIFTI_FILE found. Edit the script CONFIG paths and try again.\")\n"
      ],
      "metadata": {
        "id": "0OhFNgaTePyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49dd410-f0e4-4e51-ba2d-6dc219a3bcf0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Slice Attention Ranking ===\n",
            "Slice 1: /content/drive/MyDrive/Project_A/Test_images/verymildDem948.jpg  --> Attention Weight = 0.0355\n",
            "Slice 2: /content/drive/MyDrive/Project_A/Test_images/verymildDem949.jpg  --> Attention Weight = 0.1102\n",
            "Slice 3: /content/drive/MyDrive/Project_A/Test_images/verymildDem95.jpg  --> Attention Weight = 0.2304\n",
            "Slice 4: /content/drive/MyDrive/Project_A/Test_images/verymildDem96.jpg  --> Attention Weight = 0.3449\n",
            "Slice 5: /content/drive/MyDrive/Project_A/Test_images/verymildDem97.jpg  --> Attention Weight = 0.1269\n",
            "Slice 6: /content/drive/MyDrive/Project_A/Test_images/verymildDem98.jpg  --> Attention Weight = 0.0804\n",
            "Slice 7: /content/drive/MyDrive/Project_A/Test_images/verymildDem99.jpg  --> Attention Weight = 0.0718\n",
            "\n",
            "=== Ranking (most -> least) ===\n",
            "Rank 1: /content/drive/MyDrive/Project_A/Test_images/verymildDem96.jpg  (Weight = 0.3449)\n",
            "Rank 2: /content/drive/MyDrive/Project_A/Test_images/verymildDem95.jpg  (Weight = 0.2304)\n",
            "Rank 3: /content/drive/MyDrive/Project_A/Test_images/verymildDem97.jpg  (Weight = 0.1269)\n",
            "Rank 4: /content/drive/MyDrive/Project_A/Test_images/verymildDem949.jpg  (Weight = 0.1102)\n",
            "Rank 5: /content/drive/MyDrive/Project_A/Test_images/verymildDem98.jpg  (Weight = 0.0804)\n",
            "Rank 6: /content/drive/MyDrive/Project_A/Test_images/verymildDem99.jpg  (Weight = 0.0718)\n",
            "Rank 7: /content/drive/MyDrive/Project_A/Test_images/verymildDem948.jpg  (Weight = 0.0355)\n",
            "\n",
            "*** MOST INFORMATIVE SLICE ***\n",
            "Image: /content/drive/MyDrive/Project_A/Test_images/verymildDem96.jpg\n",
            "Attention Weight: 0.3449\n",
            "\n",
            "Folder prediction: AD\n",
            "Probabilities: [0.9888186  0.00430453 0.00687687]\n",
            "Saved attention images to: ./inference_outputs\n"
          ]
        }
      ]
    }
  ]
}